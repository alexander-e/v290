---
title: 'Robustness quantification: a new method for assessing the reliability of the
  predictions of a classifier'
abstract: Based on existing ideas in the field of imprecise probabilities, we present
  a new approach for assessing the reliability of the individual predictions of a
  generative probabilistic classifier. We call this approach robustness quantification,
  compare it to uncertainty quantification, and demonstrate that it continues to work
  well even for classifiers that are learned from small training sets that are sampled
  from a shifted distribution.
keywords: robustness quantification, classification, reliability, distribution shift,
  small data sets, imprecise probabilities
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: detavernier25a
month: 0
tex_title: 'Robustness quantification: a new method for assessing the reliability
  of the predictions of a classifier'
firstpage: 126
lastpage: 136
page: 126-136
order: 126
cycles: false
bibtex_author: Detavernier, Adri\'an and De Bock, Jasper
author:
- given: Adri√°n
  family: Detavernier
- given: Jasper
  family: De Bock
date: 2025-05-20
address:
container-title: 'Proceedings of the Fourteenth International Symposium on Imprecise
  Probabilities: Theories and Applications'
volume: '290'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 5
  - 20
pdf: https://raw.githubusercontent.com/mlresearch/v290/main/assets/detavernier25a/detavernier25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
